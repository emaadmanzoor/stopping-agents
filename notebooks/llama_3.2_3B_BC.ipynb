{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12687e07",
   "metadata": {},
   "source": [
    "# Llama 3.2 3B + Behavioral Cloning\n",
    "\n",
    "*Contributed by [Emaad Manzoor](https://emaadmanzoor.com/) (emaadmanzoor@cornell.edu) to the [stopping-agents](https://github.com/emaadmanzoor/stopping-agents) repository.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfef9d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Before running this notebook locally, you need to [install PyTorch](https://pytorch.org/get-started/locally/) for your hardware.\n",
    "\n",
    "Then, you need to install the following packages:\n",
    "\n",
    "   * transformers\n",
    "   * datasets\n",
    "   * accelerate\n",
    "   * pandas\n",
    "   * huggingface_hub (needed for Llama models)\n",
    "   * scikit-learn\n",
    "   * numpy\n",
    "\n",
    "You an also use the `requirements.txt` in the [stopping-agents](https://github.com/emaadmanzoor/stopping-agents) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import huggingface_hub # needed for Llama models\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "HF_TOKEN = \"HF_TOKEN\"\n",
    "\n",
    "COST_PER_UNIT_TIME = 0.1\n",
    "BENEFIT_PER_POSITIVE_OUTCOME = 10.0\n",
    "DECISION_OPPORTUNITIES = [45, 60] # time in seconds at which the \n",
    "                                  # agent can decide to quit or wait\n",
    "                                  # code is tailored to just 2 right now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931864dc",
   "metadata": {},
   "source": [
    "## Load and process conversation data\n",
    "\n",
    "We load a dataset of synthetic conversations available\n",
    "in the `datasets` folder at [https://github.com/emaadmanzoor/stopping-agents/](https://github.com/emaadmanzoor/stopping-agents/). This example dataset is formatted in the PyAnnote diarized conversation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c5e357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>text</th>\n",
       "      <th>outcome</th>\n",
       "      <th>is_sale</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20756_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>6.03</td>\n",
       "      <td>Hello, is this Mr. Harris? My name is Leah fro...</td>\n",
       "      <td>no sale</td>\n",
       "      <td>0</td>\n",
       "      <td>62.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20756_1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.36</td>\n",
       "      <td>7.59</td>\n",
       "      <td>Yes, speaking. I’m alright, thanks. Can I ask ...</td>\n",
       "      <td>no sale</td>\n",
       "      <td>0</td>\n",
       "      <td>62.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20756_1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.98</td>\n",
       "      <td>12.84</td>\n",
       "      <td>Of course, thanks for asking. I’m reaching out...</td>\n",
       "      <td>no sale</td>\n",
       "      <td>0</td>\n",
       "      <td>62.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20756_1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.14</td>\n",
       "      <td>15.50</td>\n",
       "      <td>Alright… I guess I can listen for a minute.</td>\n",
       "      <td>no sale</td>\n",
       "      <td>0</td>\n",
       "      <td>62.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20756_1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.89</td>\n",
       "      <td>22.14</td>\n",
       "      <td>Thank you! So, our new BrightSaver plan locks ...</td>\n",
       "      <td>no sale</td>\n",
       "      <td>0</td>\n",
       "      <td>62.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conversation_id  speaker_id  start_time  end_time  \\\n",
       "0         20756_1           0        1.25      6.03   \n",
       "1         20756_1           1        6.36      7.59   \n",
       "2         20756_1           0        7.98     12.84   \n",
       "3         20756_1           1       13.14     15.50   \n",
       "4         20756_1           0       15.89     22.14   \n",
       "\n",
       "                                                text  outcome  is_sale  \\\n",
       "0  Hello, is this Mr. Harris? My name is Leah fro...  no sale        0   \n",
       "1  Yes, speaking. I’m alright, thanks. Can I ask ...  no sale        0   \n",
       "2  Of course, thanks for asking. I’m reaching out...  no sale        0   \n",
       "3        Alright… I guess I can listen for a minute.  no sale        0   \n",
       "4  Thank you! So, our new BrightSaver plan locks ...  no sale        0   \n",
       "\n",
       "   duration  \n",
       "0     62.07  \n",
       "1     62.07  \n",
       "2     62.07  \n",
       "3     62.07  \n",
       "4     62.07  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/emaadmanzoor/stopping-agents/refs/heads/main/datasets/synthetic_sales_conversations.csv?token=GHSAT0AAAAAADBUAD4WOA6XRF2GSIX5UC4Y2EEF66Q\"\n",
    "\n",
    "diarized_conversations = pd.read_csv(dataset_url)\n",
    "\n",
    "diarized_conversations[\"is_sale\"] =\\\n",
    "        diarized_conversations[\"outcome\"].apply(\n",
    "            lambda x: 1 if x == \"sale\" else 0 if x == \"no sale\" else np.nan)\n",
    "\n",
    "diarized_conversations[\"duration\"] =\\\n",
    "    diarized_conversations.groupby(\"conversation_id\")[\"end_time\"].transform(\"max\")\n",
    "\n",
    "diarized_conversations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd33a2",
   "metadata": {},
   "source": [
    "### Split into train, validation, and test conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51e8881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1903 train conversations.\n",
      "651 validation conversations.\n",
      "860 test conversations.\n"
     ]
    }
   ],
   "source": [
    "all_conversation_ids =\\\n",
    "    diarized_conversations[[\"conversation_id\", \"is_sale\"]].drop_duplicates()[\"conversation_id\"]\\\n",
    "        .values\n",
    "all_outcomes =\\\n",
    "    diarized_conversations[[\"conversation_id\", \"is_sale\"]].drop_duplicates()[\"is_sale\"].values\n",
    "    \n",
    "train_conversation_ids, test_conversation_ids, train_outcomes, test_outcomes =\\\n",
    "    train_test_split(all_conversation_ids, all_outcomes, test_size=0.25, random_state=42,\n",
    "                     stratify=all_outcomes)\n",
    "train_conversation_ids, val_conversation_ids, train_outcomes, val_outcomes =\\\n",
    "    train_test_split(train_conversation_ids, train_outcomes, test_size=0.25, random_state=42,\n",
    "                     stratify=train_outcomes)\n",
    "\n",
    "diarized_conversations_train =\\\n",
    "    diarized_conversations[diarized_conversations[\"conversation_id\"].isin(train_conversation_ids)]\n",
    "diarized_conversations_val =\\\n",
    "    diarized_conversations[diarized_conversations[\"conversation_id\"].isin(val_conversation_ids)]\n",
    "diarized_conversations_test =\\\n",
    "    diarized_conversations[diarized_conversations[\"conversation_id\"].isin(test_conversation_ids)]\n",
    "\n",
    "print(len(diarized_conversations_train), \"train conversations.\")\n",
    "print(len(diarized_conversations_val), \"validation conversations.\")\n",
    "print(len(diarized_conversations_test), \"test conversations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ac626",
   "metadata": {},
   "source": [
    "### Accumulate transcripts at each decision opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbb7a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 train conversations.\n",
      "38 validation conversations.\n",
      "50 test conversations.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>is_sale</th>\n",
       "      <th>transcript_speaker_45</th>\n",
       "      <th>transcript_speaker_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20756_1</td>\n",
       "      <td>62.07</td>\n",
       "      <td>0</td>\n",
       "      <td>Speaker 0: Hello, is this Mr. Harris? My name ...</td>\n",
       "      <td>Speaker 0: Hello, is this Mr. Harris? My name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59321_6</td>\n",
       "      <td>55.37</td>\n",
       "      <td>0</td>\n",
       "      <td>Speaker 0: Good afternoon! Is this Ms. Parker?...</td>\n",
       "      <td>Speaker 0: Good afternoon! Is this Ms. Parker?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92837_7</td>\n",
       "      <td>43.64</td>\n",
       "      <td>0</td>\n",
       "      <td>Speaker 0: Good afternoon, may I speak with Mr...</td>\n",
       "      <td>Speaker 0: Good afternoon, may I speak with Mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58241_9</td>\n",
       "      <td>50.01</td>\n",
       "      <td>0</td>\n",
       "      <td>Speaker 0: Hello, may I speak with Ms. Jenkins...</td>\n",
       "      <td>Speaker 0: Hello, may I speak with Ms. Jenkins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20567_11</td>\n",
       "      <td>45.19</td>\n",
       "      <td>0</td>\n",
       "      <td>Speaker 0: Good afternoon, is this Mr. Carver?...</td>\n",
       "      <td>Speaker 0: Good afternoon, is this Mr. Carver?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conversation_id  duration  is_sale  \\\n",
       "0         20756_1     62.07        0   \n",
       "1         59321_6     55.37        0   \n",
       "2         92837_7     43.64        0   \n",
       "3         58241_9     50.01        0   \n",
       "4        20567_11     45.19        0   \n",
       "\n",
       "                               transcript_speaker_45  \\\n",
       "0  Speaker 0: Hello, is this Mr. Harris? My name ...   \n",
       "1  Speaker 0: Good afternoon! Is this Ms. Parker?...   \n",
       "2  Speaker 0: Good afternoon, may I speak with Mr...   \n",
       "3  Speaker 0: Hello, may I speak with Ms. Jenkins...   \n",
       "4  Speaker 0: Good afternoon, is this Mr. Carver?...   \n",
       "\n",
       "                               transcript_speaker_60  \n",
       "0  Speaker 0: Hello, is this Mr. Harris? My name ...  \n",
       "1  Speaker 0: Good afternoon! Is this Ms. Parker?...  \n",
       "2  Speaker 0: Good afternoon, may I speak with Mr...  \n",
       "3  Speaker 0: Hello, may I speak with Ms. Jenkins...  \n",
       "4  Speaker 0: Good afternoon, is this Mr. Carver?...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2 = sorted(DECISION_OPPORTUNITIES)\n",
    "\n",
    "data_transcripts = {}\n",
    "for df, dftype in zip([diarized_conversations_train,\n",
    "                       diarized_conversations_val,\n",
    "                       diarized_conversations_test],\n",
    "                      [\"train\", \"val\", \"test\"]):\n",
    "    \n",
    "    data_transcripts[dftype] = df.copy()\n",
    "\n",
    "    data_transcripts[dftype][\"transcript\"] =\\\n",
    "        \"Speaker \" +\\\n",
    "        data_transcripts[dftype][\"speaker_id\"].astype(str) + \": \" +\\\n",
    "        data_transcripts[dftype][\"text\"]\n",
    "\n",
    "    transcripts = {}\n",
    "    for m in [m1, m2]: \n",
    "        transcripts[m] =\\\n",
    "            data_transcripts[dftype].loc[(data_transcripts[dftype][\"end_time\"]>=0) &\n",
    "                                         (data_transcripts[dftype][\"end_time\"]<m)]\\\n",
    "                    .groupby(\"conversation_id\")[\"transcript\"]\\\n",
    "                    .apply(lambda x: '\\n'.join(x))\\\n",
    "                    .reset_index(name=\"transcript_speaker_\" + str(m))\n",
    "\n",
    "    data_transcripts[dftype] = \\\n",
    "        pd.merge(data_transcripts[dftype][[\"conversation_id\",\n",
    "                                   \"duration\",\n",
    "                                   \"is_sale\"]].drop_duplicates(),\n",
    "                 transcripts[m1],\n",
    "                 on=\"conversation_id\", how=\"left\", validate=\"one_to_one\")\\\n",
    "                    .merge(transcripts[m2],\n",
    "                           on=\"conversation_id\", how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "print(len(data_transcripts[\"train\"]), \"train conversations.\")\n",
    "print(len(data_transcripts[\"val\"]), \"validation conversations.\")\n",
    "print(len(data_transcripts[\"test\"]), \"test conversations.\")\n",
    "\n",
    "assert len(data_transcripts[\"train\"]) +\\\n",
    "         len(data_transcripts[\"val\"]) +\\\n",
    "         len(data_transcripts[\"test\"]) == diarized_conversations[\"conversation_id\"].nunique()\n",
    "\n",
    "data_transcripts[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc4397",
   "metadata": {},
   "source": [
    "### Construct states\n",
    "\n",
    "Wrap the transcript until each decision opportunity in a prompt to construct the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a27cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example state at 45 seconds:\n",
      "Below is the first 45 seconds of the sales call between the sales agent Speaker 0 and the customer Speaker 1:\n",
      "Speaker 0: Hello, is this Mr. Harris? My name is Leah from Sunview Energy—how are you today?\n",
      "Speaker 1: Yes, speaking. I’m alright, thanks. Can I ask what this is about?\n",
      "Speaker 0: Of course, thanks for asking. I’m reaching out because we’re offering a new energy plan that could qualify you for a 15% discount on your electric bill. I wanted to see if I could quickly tell you about it.\n",
      "Speaker 1: Alright… I guess I can listen for a minute.\n",
      "Speaker 0: Thank you! So, our new BrightSaver plan locks in your rate for twelve months—there’s no change in price based on the time of day, and there are no hidden fees. And for this month, you’d also get an automatic 15% off your supply charges.\n",
      "Speaker 1: Is this something I have to switch providers for? I’m pretty happy with who I have now.\n",
      "Speaker 0: You would stay connected to your local utility for service and repairs, but Sunview would handle the billing and supply. The switch is very simple and risk-free—if you change your mind, you can cancel within 30 days.\n",
      "Speaker 1: I see. Is there a contract or any penalties?\n",
      "Speaker 0: There's no early cancellation fee and no long-term contract; you can opt out any time. We just want people to enjoy lower, predictable pricing with no risk.\n",
      "Speaker 1: To be honest, I just re-upped my plan last month. I don’t like to change stuff if it’s working.\n",
      "Will this call end in a sale (respond with 'yes' or 'no'):  \n",
      "\n",
      "Example state at 60 seconds:\n",
      "Below is the first 60 seconds of the sales call between the sales agent Speaker 0 and the customer Speaker 1:\n",
      "Speaker 0: Hello, is this Mr. Harris? My name is Leah from Sunview Energy—how are you today?\n",
      "Speaker 1: Yes, speaking. I’m alright, thanks. Can I ask what this is about?\n",
      "Speaker 0: Of course, thanks for asking. I’m reaching out because we’re offering a new energy plan that could qualify you for a 15% discount on your electric bill. I wanted to see if I could quickly tell you about it.\n",
      "Speaker 1: Alright… I guess I can listen for a minute.\n",
      "Speaker 0: Thank you! So, our new BrightSaver plan locks in your rate for twelve months—there’s no change in price based on the time of day, and there are no hidden fees. And for this month, you’d also get an automatic 15% off your supply charges.\n",
      "Speaker 1: Is this something I have to switch providers for? I’m pretty happy with who I have now.\n",
      "Speaker 0: You would stay connected to your local utility for service and repairs, but Sunview would handle the billing and supply. The switch is very simple and risk-free—if you change your mind, you can cancel within 30 days.\n",
      "Speaker 1: I see. Is there a contract or any penalties?\n",
      "Speaker 0: There's no early cancellation fee and no long-term contract; you can opt out any time. We just want people to enjoy lower, predictable pricing with no risk.\n",
      "Speaker 1: To be honest, I just re-upped my plan last month. I don’t like to change stuff if it’s working.\n",
      "Speaker 0: That’s completely understandable, Mr. Harris. Do you mind if I ask how much you’re paying per kilowatt-hour, just to make sure you’re on the best deal?\n",
      "Speaker 1: Actually, I’m not sure off the top of my head. I just check that the total seems right each month.\n",
      "Speaker 0: Totally fair. If you’re interested, I could email you a side-by-side comparison of our BrightSaver plan and your last bill—no obligation, just information.\n",
      "Speaker 1: No, that’s okay. If I decide to look into it, I’ll reach out myself.\n",
      "Will this call end in a sale (respond with 'yes' or 'no'):  \n"
     ]
    }
   ],
   "source": [
    "def convert_to_state(transcript, t):\n",
    "    assert type(transcript) == str\n",
    "\n",
    "    state = \"Below is the first \" + str(t) +\\\n",
    "            \" seconds of the sales call between the sales agent Speaker 0 and\" +\\\n",
    "            \" the customer Speaker 1:\\n\" +\\\n",
    "            transcript + \"\\n\" +\\\n",
    "            \"Will this call end in a sale (respond with 'yes' or 'no'):  \"\n",
    "\n",
    "    return state\n",
    "\n",
    "for df in [data_transcripts[\"train\"],\n",
    "           data_transcripts[\"val\"],\n",
    "           data_transcripts[\"test\"]]:\n",
    "\n",
    "    for m in [m1, m2]:\n",
    "        df.loc[:, \"s\" + str(m)] = df.apply(lambda x:\n",
    "                                            convert_to_state(x[\"transcript_speaker_\" + str(m)],\n",
    "                                                             m), axis=1)\n",
    "\n",
    "print(\"Example state at\", m1, \"seconds:\")\n",
    "print(data_transcripts[\"train\"][\"s\" + str(m1)].values[0])\n",
    "print()\n",
    "print(\"Example state at\", m2, \"seconds:\")\n",
    "print(data_transcripts[\"train\"][\"s\" + str(m2)].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed11db",
   "metadata": {},
   "source": [
    "### Construct optimal state-action pairs\n",
    "\n",
    "This is the most important part of the behavioral cloning dataset construction process. Given the `COST_PER_UNIT_TIME` and `BENEFIT_PER_POSITIVE_OUTCOME`, the code below maps the conversation transcript before each decision opportunity to the optimal action (`wait` or `quit`) to take in that state. The optimal action is the one that maximizes the cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "428eb3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of actions in the optimal state-action pairs:\n",
      "action\n",
      "yes    108\n",
      "no      67\n",
      "Name: count, dtype: int64\n",
      "action\n",
      "yes    38\n",
      "no     22\n",
      "Name: count, dtype: int64\n",
      "action\n",
      "yes    49\n",
      "no     30\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>is_sale</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20756_1</td>\n",
       "      <td>Below is the first 45 seconds of the sales cal...</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>62.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59321_6</td>\n",
       "      <td>Below is the first 45 seconds of the sales cal...</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>55.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58241_9</td>\n",
       "      <td>Below is the first 45 seconds of the sales cal...</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>50.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20567_11</td>\n",
       "      <td>Below is the first 45 seconds of the sales cal...</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>45.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10523_13</td>\n",
       "      <td>Below is the first 45 seconds of the sales cal...</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>65.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conversation_id                                              state action  \\\n",
       "0         20756_1  Below is the first 45 seconds of the sales cal...     no   \n",
       "1         59321_6  Below is the first 45 seconds of the sales cal...     no   \n",
       "2         58241_9  Below is the first 45 seconds of the sales cal...     no   \n",
       "3        20567_11  Below is the first 45 seconds of the sales cal...     no   \n",
       "4        10523_13  Below is the first 45 seconds of the sales cal...     no   \n",
       "\n",
       "   is_sale  duration  \n",
       "0        0     62.07  \n",
       "1        0     55.37  \n",
       "2        0     50.01  \n",
       "3        0     45.19  \n",
       "4        0     65.04  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_state_action_pairs = {}\n",
    "for dftype, df in data_transcripts.items():\n",
    "    df[\"rq\" + str(m1)] = -m1 * COST_PER_UNIT_TIME # stop at 30\n",
    "\n",
    "    # continue at 30, stop at 60\n",
    "    df[\"rq\" + str(m2)] = df[\"is_sale\"].astype(int)\\\n",
    "                        * BENEFIT_PER_POSITIVE_OUTCOME\\\n",
    "                        * (df[\"duration\"]<=m2).astype(int) \\\n",
    "                        - df[\"duration\"].apply(lambda x: min(m2, x)) * COST_PER_UNIT_TIME\n",
    "    \n",
    "    # continue at 30, continue at 60, continue at 90 = never quit                \n",
    "    df[\"rc\" + str(m2)] = df[\"is_sale\"].astype(int) * BENEFIT_PER_POSITIVE_OUTCOME\\\n",
    "                        - df[\"duration\"] * COST_PER_UNIT_TIME\n",
    "\n",
    "    df[\"max_reward\"] = df[[\"rq\" + str(m1), \"rq\" + str(m2), \"rc\" + str(m2)]].max(axis=1)\n",
    "\n",
    "    # optimal to quit at 30\n",
    "    df.loc[df[\"max_reward\"]==df[\"rq\" + str(m1)], \"a\" + str(m1)] = \"no\"\n",
    "    df.loc[df[\"max_reward\"]==df[\"rq\" + str(m1)], \"a\" + str(m2)] = \"no\"\n",
    "\n",
    "    # optimal to continue at 30, stop at 60\n",
    "    df.loc[df[\"max_reward\"]==df[\"rq\" + str(m2)], \"a\"  + str(m1)] = \"yes\"\n",
    "    df.loc[df[\"max_reward\"]==df[\"rq\" + str(m2)], \"a\"  + str(m2)] = \"no\"\n",
    "\n",
    "    # optimal to continue at 30, continue at 60, continue at 90\n",
    "    df.loc[df[\"max_reward\"]==df[\"rc\" + str(m2)], \"a\" + str(m1)] = \"yes\"\n",
    "    df.loc[df[\"max_reward\"]==df[\"rc\" + str(m2)], \"a\" + str(m2)] = \"yes\"\n",
    "\n",
    "    optimal_state_action_pairs[dftype] = []\n",
    "    \n",
    "    for m in [m1, m2]:\n",
    "        optimal_actions = df[df[\"duration\"]>=m]\\\n",
    "                                [[\"conversation_id\", \"s\" + str(m), \"a\" + str(m), \"is_sale\", \n",
    "                                  \"duration\"]]\n",
    "        optimal_actions = optimal_actions.values.tolist()\n",
    "        optimal_state_action_pairs[dftype].extend(optimal_actions)\n",
    "\n",
    "    optimal_state_action_pairs[dftype] =\\\n",
    "        pd.DataFrame(optimal_state_action_pairs[dftype],\n",
    "                     columns=[\"conversation_id\", \"state\", \"action\",\n",
    "                              \"is_sale\", \"duration\"])\n",
    "\n",
    "print(\"Distribution of actions in the optimal state-action pairs:\")\n",
    "print(optimal_state_action_pairs[\"train\"][\"action\"].value_counts())\n",
    "print(optimal_state_action_pairs[\"val\"][\"action\"].value_counts())\n",
    "print(optimal_state_action_pairs[\"test\"][\"action\"].value_counts())\n",
    "if len(optimal_state_action_pairs[\"train\"][\"action\"].value_counts()) == 1:\n",
    "    print(\"Only one optimal action in the training set; imitation learning will fail.\")\n",
    "\n",
    "optimal_state_action_pairs[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab086ec4",
   "metadata": {},
   "source": [
    "## Behavioral Cloning\n",
    "\n",
    "Now we can fine-tune our large language model to generate the optimal action for every state.\n",
    "\n",
    "We fine-tune using `Trainer` in `Transformers` library instead of `SFTTRainer` in `TRL` for 2 reasons:\n",
    "\n",
    "   1. By default, `SFTTrainer` calculates the loss for *all* tokens, including ones in the prompt. One way to work around this using the prompt-completion pairs data format.\n",
    "\n",
    "   2. `SFTTrainer` does not enable custom metrics. We want to report the action predicton validation AUC during training, since it is more correlated with our final goal than the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb8163",
   "metadata": {},
   "source": [
    "### Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db06d469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa0faf14d8d43bcaea305837ff480f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.login(token=HF_TOKEN)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B\" # base model, not instruction-tuned\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "if len(tokenizer) > model.get_input_embeddings().weight.shape[0]:\n",
    "    print(\"WARNING: Resizing the embedding matrix to match the tokenizer vocab size.\")\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240767e7",
   "metadata": {},
   "source": [
    "### Construct and tokenize fine-tuning datasets\n",
    "\n",
    "We perform manual masking, so the loss is only calculated for the generated actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c94248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac592b7caaa94bc4b89d7fb9d868dacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3778f08ca3034ddc979b073ee5447088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization test:\n",
      "<|begin_of_text|>Below is the first 45 seconds of the sales call between the sales agent Speaker 0 and the customer Speaker 1:\n",
      "Speaker 0: Good afternoon, this is Marcus from Greenwave Energy. Am I speaking with Ms. Lopez?\n",
      "Speaker 1: Hi, yes, this is her.\n",
      "Speaker 0: Fantastic! I’ll keep this brief. We have a new energy plan with a guaranteed rate and monthly discounts for loyal clients. Are you open to hearing a quick summary?\n",
      "Speaker 1: Alright, sure. Go ahead.\n",
      "Speaker 0: Thank you! With the Greenwave Saver Plan, you lock in a fixed rate on electricity for a year. We're offering a $7 discount each month on your bill and a one-time $30 sign-up bonus. All this, with no contract lock-in or exit fees.\n",
      "Speaker 1: Hmm. What's the rate compared to what I'm paying now?\n",
      "Speaker 0: Great question. On your most recent bill, you were charged $0.15 per kWh. Our plan offers $0.132 per kWh, so you'd see a savings, plus the ongoing monthly discount.\n",
      "Speaker 1: I don’t know… It sounds good, but I just switched providers last month. I’m kind of locked in for now.\n",
      "Speaker 0: I understand. If there’s a penalty to leave early, I’d hate for you to pay that. Just so you know, our plan has no switching fee, so you could come back anytime.\n",
      "Speaker 1: That’s helpful, thank you. But I think for now, I’ll have to pass. Maybe I’ll look into it when my contact runs out.\n",
      "Will this call end in a sale (respond with 'yes' or 'no'):  no<|end_of_text|>\n",
      "\n",
      "Expected Label (action):\n",
      "no<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.Dataset.from_dict(\n",
    "    {\"prompt\": [state for state in optimal_state_action_pairs[\"train\"][\"state\"].values], \n",
    "     \"completion\": [action.strip()\n",
    "                    for action in optimal_state_action_pairs[\"train\"][\"action\"].values]}).shuffle()\n",
    "    \n",
    "val_dataset = datasets.Dataset.from_dict(\n",
    "    {\"prompt\": [state for state in optimal_state_action_pairs[\"val\"][\"state\"].values],\n",
    "     \"completion\": [action.strip()\n",
    "                    for action in optimal_state_action_pairs[\"val\"][\"action\"].values]}).shuffle()\n",
    "\n",
    "def tokenize_fn(example, add_label):\n",
    "    # start with the BOS token if it exists\n",
    "    if tokenizer.bos_token is not None:\n",
    "        encoded_prompt = tokenizer.encode(tokenizer.bos_token +\n",
    "                                          example[\"prompt\"],              \n",
    "                                          add_special_tokens=False)\n",
    "    else:\n",
    "        encoded_prompt = tokenizer.encode(example[\"prompt\"], \n",
    "                                          add_special_tokens=False)\n",
    "\n",
    "    # add the label if needed for the training and validation datasets\n",
    "    if add_label:\n",
    "        encoded_label = tokenizer.encode(example[\"completion\"] + tokenizer.eos_token, \n",
    "                                         add_special_tokens=False)\n",
    "        return {\"input_ids\": encoded_prompt + encoded_label,\n",
    "                \"attention_mask\" : [1] * (len(encoded_prompt) + len(encoded_label)),\n",
    "                \"labels\": [-100] * len(encoded_prompt) + encoded_label}\n",
    "    else:\n",
    "        return {\"input_ids\": encoded_prompt,\n",
    "                \"attention_mask\": [1] * len(encoded_prompt),\n",
    "                \"labels\": [-100] * len(encoded_prompt)}\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn,\n",
    "                                  remove_columns=[\"prompt\", \"completion\"], \n",
    "                                  fn_kwargs={\"add_label\": True})\n",
    "val_dataset = val_dataset.map(tokenize_fn, \n",
    "                              remove_columns=[\"prompt\", \"completion\"], \n",
    "                              fn_kwargs={\"add_label\": True})\n",
    "\n",
    "print(\"Tokenization test:\")\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"]))\n",
    "print()\n",
    "print(\"Expected Label (action):\")\n",
    "print(tokenizer.decode([l for l in train_dataset[0][\"labels\"] if l!=-100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5717d6",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df59ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 75/150 02:23 < 02:27, 0.51 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.727800</td>\n",
       "      <td>0.395835</td>\n",
       "      <td>0.083732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.319866</td>\n",
       "      <td>0.556220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.225835</td>\n",
       "      <td>0.952153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.329565</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=1.3294019158681234, metrics={'train_runtime': 144.6715, 'train_samples_per_second': 12.096, 'train_steps_per_second': 1.037, 'total_flos': 7403051037947904.0, 'train_loss': 1.3294019158681234, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YES_TOKEN_ID = tokenizer.encode(\"yes\", add_special_tokens=False)[-1]\n",
    "NO_TOKEN_ID = tokenizer.encode(\"no\", add_special_tokens=False)[-1]\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    return logits[:, -3, :] # last non-padding token logits only, for causal LM\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    labels = [l[(l != -100) & (l != tokenizer.pad_token_id)][0] for l in labels]\n",
    "\n",
    "    logprobs = [torch.log_softmax(torch.from_numpy(s), dim=-1).numpy()\n",
    "                for s in logits]\n",
    "    labelprobs = [math.exp(logprob[label]) for logprob, label in zip(logprobs, labels)]\n",
    "\n",
    "    ytrue = [1 if label == YES_TOKEN_ID else 0 for label in labels]\n",
    "    ypred = [labelprob if label==YES_TOKEN_ID else 1.0 - labelprob\n",
    "             for labelprob, label in zip(labelprobs, labels)]\n",
    "    auc = roc_auc_score(ytrue, ypred)\n",
    "\n",
    "    return {\"auc\": auc}\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./llama-3.2-3B/\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    save_strategy=\"best\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "\n",
    "    # on 1 H100 96GB: batch size of 12-16 for 3B works\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    optim=\"adamw_torch\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auc\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    report_to=\"none\", # change to wandb if needed\n",
    "    save_safetensors=False, # needed to load saved models\n",
    "\n",
    "    # change to suit hardware\n",
    "    bf16=True, \n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset.shuffle(),\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=transformers.data.DataCollatorForSeq2Seq(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c26914",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb95891",
   "metadata": {},
   "source": [
    "#### Get the validation and test responses for each state\n",
    "\n",
    "We get the agent's responses for the validation and test conversations. We use the validation responses for backward-induction threshold-tuning (our scalable alternative to grid search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88183bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535122a54f004677a68c0156bad0e514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting validation responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a699eea10245818047fa37bed6ec25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_prompts = list(optimal_state_action_pairs[\"val\"][\"state\"].values)\n",
    "test_prompts = list(optimal_state_action_pairs[\"test\"][\"state\"].values)\n",
    "\n",
    "print(\"Getting test responses...\")\n",
    "\n",
    "responses_test = []\n",
    "logprobs_test = []\n",
    "batch_size = 72 # change to suit hardware\n",
    "\n",
    "for i in tqdm(range(0, len(test_prompts), batch_size)):\n",
    "    batch_prompts = test_prompts[i:i+batch_size]\n",
    "    batch = tokenizer(batch_prompts, \n",
    "                      return_tensors=\"pt\", \n",
    "                      padding=True, \n",
    "                      add_special_tokens=True,\n",
    "                      truncation=True).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model.generate(\n",
    "            **batch, \n",
    "            max_new_tokens=2, \n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=None, top_p=None, top_k=None,\n",
    "            return_dict_in_generate=True, output_scores=True\n",
    "            # greedy decoding: so output_scores = output_logits\n",
    "        )\n",
    "\n",
    "    seqs = outputs.sequences\n",
    "    prompt_len = batch['input_ids'].shape[1]   # Length of the input prompts\n",
    "\n",
    "    # Slice to get only the generated new tokens\n",
    "    generated_tokens = seqs[:, prompt_len:]\n",
    "    decoded_outputs = tokenizer.batch_decode(generated_tokens,\n",
    "                                             skip_special_tokens=True)\n",
    "    decoded_outputs = [d.strip().lower() for d in decoded_outputs]\n",
    "\n",
    "    responses_test.extend(decoded_outputs)\n",
    "\n",
    "    scores = outputs.scores\n",
    "    logprobs = [torch.log_softmax(s, dim=-1) for s in scores]\n",
    "    logprobs = logprobs[0][torch.arange(logprobs[0].size(0)), seqs[:, -2].view(-1)]\n",
    "    logprobs_test.extend(logprobs.cpu().numpy())\n",
    "\n",
    "print(\"Getting validation responses...\")\n",
    "\n",
    "responses_val = []\n",
    "logprobs_val = []\n",
    "batch_size = 72\n",
    "\n",
    "for i in tqdm(range(0, len(val_prompts), batch_size)):\n",
    "    batch_prompts = val_prompts[i:i+batch_size]\n",
    "    batch = tokenizer(batch_prompts, \n",
    "                      return_tensors=\"pt\", \n",
    "                      padding=True, \n",
    "                      truncation=True).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model.generate(\n",
    "            **batch, \n",
    "            max_new_tokens=2, \n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=None, top_p=None, top_k=None,\n",
    "            return_dict_in_generate=True, output_scores=True\n",
    "        )\n",
    "\n",
    "    seqs = outputs.sequences\n",
    "    prompt_len = batch['input_ids'].shape[1]   # Length of the input prompts\n",
    "\n",
    "    # Slice to get only the generated new tokens\n",
    "    generated_tokens = seqs[:, prompt_len:]\n",
    "    decoded_outputs = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    decoded_outputs = [d.strip().lower() for d in decoded_outputs]\n",
    "\n",
    "    responses_val.extend(decoded_outputs)\n",
    "\n",
    "    scores = outputs.scores\n",
    "    logprobs = [torch.log_softmax(s, dim=-1) for s in scores]\n",
    "    logprobs = logprobs[0][torch.arange(logprobs[0].size(0)), seqs[:, -2].view(-1)]\n",
    "    logprobs_val.extend(logprobs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c7a682",
   "metadata": {},
   "source": [
    "#### Store validation and test responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "772d7131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val 45 ROC-AUC: 1.00\n",
      "Test 45 ROC-AUC: 0.99\n",
      "Val 60 ROC-AUC: 1.00\n",
      "Test 60 ROC-AUC: 0.98\n"
     ]
    }
   ],
   "source": [
    "predictions = optimal_state_action_pairs[\"test\"].copy()\n",
    "predictions[\"response\"] = responses_test\n",
    "predictions[\"logprob\"] = logprobs_test\n",
    "predictions[\"prob\"] = predictions[\"logprob\"].apply(lambda x: math.exp(x))\n",
    "predictions.loc[predictions[\"response\"]==\"yes\", \"prob_yes\"] =\\\n",
    "    predictions.loc[predictions[\"response\"]==\"yes\", \"prob\"]\n",
    "predictions.loc[predictions[\"response\"]!=\"yes\", \"prob_yes\"] =\\\n",
    "    1.0 - predictions.loc[predictions[\"response\"]!=\"yes\", \"prob\"]\n",
    "\n",
    "predictions_val = optimal_state_action_pairs[\"val\"].copy()\n",
    "predictions_val[\"response\"] = responses_val\n",
    "predictions_val[\"logprob\"] = logprobs_val\n",
    "predictions_val[\"prob\"] =\\\n",
    "    predictions_val[\"logprob\"].apply(lambda x: math.exp(x))\n",
    "predictions_val.loc[predictions_val[\"response\"]==\"yes\", \"prob_yes\"] =\\\n",
    "    predictions_val.loc[predictions_val[\"response\"]==\"yes\", \"prob\"]\n",
    "predictions_val.loc[predictions_val[\"response\"]!=\"yes\", \"prob_yes\"] =\\\n",
    "    1.0 - predictions_val.loc[predictions_val[\"response\"]!=\"yes\", \"prob\"]\n",
    "\n",
    "test_with_predictions  = pd.merge(left=data_transcripts[\"test\"],\n",
    "                                     right=predictions[[\"conversation_id\",\n",
    "                                                        \"state\", \"prob_yes\"]],\n",
    "                                     left_on=[\"conversation_id\", \"s\" + str(m1)],\n",
    "                                     right_on=[\"conversation_id\", \"state\"],\n",
    "                                     how=\"left\", validate=\"one_to_one\")\\\n",
    "                                        .drop(columns=[\"state\"])\\\n",
    "                                        .rename(columns={\"prob_yes\": \"prob_yes_\" + str(m1)})\\\n",
    "                              .merge(right=predictions[[\"conversation_id\",\n",
    "                                                        \"state\", \"prob_yes\"]],\n",
    "                                     left_on=[\"conversation_id\", \"s\" + str(m2)],\n",
    "                                     right_on=[\"conversation_id\", \"state\"],\n",
    "                                     how=\"left\", validate=\"one_to_one\")\\\n",
    "                                        .drop(columns=[\"state\"])\\\n",
    "                                        .rename(columns={\"prob_yes\": \"prob_yes_\" + str(m2)})\\\n",
    "\n",
    "test_with_predictions.loc[\n",
    "    test_with_predictions[\"prob_yes_\" + str(m1)].isnull(),\n",
    "                          \"prob_yes_\" + str(m1)] = 0\n",
    "test_with_predictions.loc[\n",
    "    test_with_predictions[\"prob_yes_\" + str(m2)].isnull(),\n",
    "                          \"prob_yes_\" + str(m2)] = 0\n",
    "\n",
    "val_with_predictions  = pd.merge(left=data_transcripts[\"val\"],\n",
    "                                    right=predictions_val[[\"conversation_id\",\n",
    "                                         \"state\", \"prob_yes\"]],\n",
    "                                    left_on=[\"conversation_id\", \"s\" + str(m1)],\n",
    "                                    right_on=[\"conversation_id\", \"state\"],\n",
    "                                    how=\"left\", validate=\"one_to_one\")\\\n",
    "                                        .drop(columns=[\"state\"])\\\n",
    "                                        .rename(columns={\"prob_yes\": \"prob_yes_\" + str(m1)})\\\n",
    "                             .merge(right=predictions_val[[\"conversation_id\", \n",
    "                                         \"state\", \"prob_yes\"]],\n",
    "                                    left_on=[\"conversation_id\", \"s\" + str(m2)],\n",
    "                                    right_on=[\"conversation_id\", \"state\"],\n",
    "                                    how=\"left\", validate=\"one_to_one\")\\\n",
    "                                        .drop(columns=[\"state\"])\\\n",
    "                                        .rename(columns={\"prob_yes\": \"prob_yes_\" + str(m2)})\\\n",
    "\n",
    "val_with_predictions.loc[\n",
    "    val_with_predictions[\"prob_yes_\" + str(m1)].isnull(),\n",
    "                         \"prob_yes_\" + str(m1)] = 0\n",
    "val_with_predictions.loc[\n",
    "    val_with_predictions[\"prob_yes_\" + str(m2)].isnull(),\n",
    "                         \"prob_yes_\" + str(m2)] = 0\n",
    "\n",
    "print(\"Val\", m1, \"ROC-AUC: {:.2f}\".format(\n",
    "    roc_auc_score(val_with_predictions[\"is_sale\"].values,\n",
    "                  val_with_predictions[\"prob_yes_\" + str(m1)].values)))\n",
    "print(\"Test\", m1, \"ROC-AUC: {:.2f}\".format(\n",
    "    roc_auc_score(test_with_predictions[\"is_sale\"].values,\n",
    "                  test_with_predictions[\"prob_yes_\" + str(m1)].values)))\n",
    "\n",
    "print(\"Val\", m2, \"ROC-AUC: {:.2f}\".format(\n",
    "    roc_auc_score(val_with_predictions[\"is_sale\"].values,\n",
    "                  val_with_predictions[\"prob_yes_\" + str(m2)].values)))\n",
    "print(\"Test\", m2, \"ROC-AUC: {:.2f}\".format(\n",
    "    roc_auc_score(test_with_predictions[\"is_sale\"].values,\n",
    "                  test_with_predictions[\"prob_yes_\" + str(m2)].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffe317",
   "metadata": {},
   "source": [
    "#### Get optimal thresholds using backward-induction threshold tuning\n",
    "\n",
    "We could do a grid search, but this is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f728823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set reward without the stopping agent:\n",
      "Total reward on test: -77.97900000000004\n",
      "Avg. reward on test: -1.5595800000000009\n",
      "Total sales on test: 25\n",
      "Total time on test (seconds): 3279.79\n"
     ]
    }
   ],
   "source": [
    "def simulate_threshold(threshold_m1, threshold_m2, df):\n",
    "    # quit at m1\n",
    "    calls_quit_at_m1 = df.loc[(df[\"prob_yes_\" + str(m1)] < threshold_m1)]\n",
    "    \n",
    "    # continue at m1, ended before m2\n",
    "    calls_continued_at_m1_and_ended = df.loc[(df[\"prob_yes_\" + str(m1)] >= threshold_m1) &\n",
    "                                             (df[\"duration\"]<m2)]\n",
    "    \n",
    "    # continued at m1, did not end before m2, quit at m2\n",
    "    calls_continued_at_m1_and_quit_at_m2 = df.loc[(df[\"prob_yes_\" + str(m1)] >= threshold_m1) &\n",
    "                                                  (df[\"prob_yes_\" + str(m2)] < threshold_m2) &\n",
    "                                                  (df[\"duration\"]>=m2)]\n",
    "    \n",
    "    # continue at m1, did not end before m2, continued at m2\n",
    "    calls_continued_at_m2 = df.loc[(df[\"prob_yes_\" + str(m1)] >= threshold_m1) &\n",
    "                                   (df[\"prob_yes_\" + str(m2)] >= threshold_m2) &\n",
    "                                   (df[\"duration\"]>=m2)]\n",
    "    \n",
    "    assert len(calls_quit_at_m1) + len(calls_continued_at_m1_and_ended) +\\\n",
    "          len(calls_continued_at_m1_and_quit_at_m2) + \\\n",
    "          len(calls_continued_at_m2) == len(df)\n",
    "\n",
    "    total_sales = calls_continued_at_m1_and_ended[\"is_sale\"].sum() +\\\n",
    "                    calls_continued_at_m2[\"is_sale\"].sum()\n",
    "    total_sales_benefit = total_sales * BENEFIT_PER_POSITIVE_OUTCOME\n",
    "\n",
    "    total_time = (len(calls_quit_at_m1) * m1 +\\\n",
    "                  len(calls_continued_at_m1_and_quit_at_m2) * m2 +\\\n",
    "                    calls_continued_at_m1_and_ended[\"duration\"].sum() +\\\n",
    "                    calls_continued_at_m2[\"duration\"].sum())\n",
    "    total_cost = total_time * COST_PER_UNIT_TIME\n",
    "    \n",
    "    total_reward = (total_sales_benefit - total_cost)\n",
    "    average_reward = total_reward/len(df)\n",
    "\n",
    "    # assert benefit.sum() == total_sales_benefit\n",
    "    # assert np.isclose(cost.sum(), total_cost)\n",
    "    # assert np.isclose(reward.sum(), total_reward)\n",
    "\n",
    "    return total_reward, average_reward, total_sales, total_time\n",
    "\n",
    "print(\"Test set reward without the stopping agent:\")\n",
    "total_reward_noagent, average_reward_noagent,\\\n",
    "    total_sales_noagent, total_time_noagent =\\\n",
    "    simulate_threshold(0, 0, test_with_predictions)\n",
    "print(\"Total reward on test:\", total_reward_noagent)\n",
    "print(\"Avg. reward on test:\", average_reward_noagent)\n",
    "print(\"Total sales on test:\", total_sales_noagent)\n",
    "print(\"Total time on test (seconds):\", total_time_noagent)\n",
    "\n",
    "assert int(total_sales_noagent) == int(test_with_predictions[\"is_sale\"].sum())\n",
    "assert total_time_noagent == test_with_predictions[\"duration\"].sum()\n",
    "assert total_reward_noagent ==\\\n",
    "    (total_sales_noagent * BENEFIT_PER_POSITIVE_OUTCOME - total_time_noagent * COST_PER_UNIT_TIME)\n",
    "assert average_reward_noagent ==\\\n",
    "    total_reward_noagent / len(test_with_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85ea3128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold at m=45: 0.00010000464725449688\n",
      "Best threshold at m=60: 0.0014000817639441545\n",
      "\n",
      "Test set reward with the stopping agent:\n",
      "Total reward on test: -60.261000000000024\n",
      "Avg. reward on test: -1.2052200000000004\n",
      "Total sales on test: 24\n",
      "Total time on test: 3002.61\n",
      "\n",
      "Comparative results:\n",
      "Sales lost by stopping agent: 1\n",
      "Time saved by stopping agent (seconds): 277.17999999999984\n",
      "Time saved by stopping agent (%): 8.451150835876682\n"
     ]
    }
   ],
   "source": [
    "best_threshold_at_m = {}\n",
    "num_grid_points = 10000\n",
    "\n",
    "m = m1\n",
    "prob_column = \"prob_yes_\" + str(m)    \n",
    "best_reward = -10000000\n",
    "for candidate_threshold in np.linspace(val_with_predictions[prob_column].min()-10**-12,\n",
    "                                       val_with_predictions[prob_column].max()+10**-12,\n",
    "                                       num=num_grid_points):\n",
    "    \n",
    "    total_reward, average_reward, total_sales, total_time =\\\n",
    "        simulate_threshold(0, candidate_threshold, val_with_predictions)\n",
    "    \n",
    "    if average_reward > best_reward:\n",
    "        best_reward = average_reward\n",
    "        best_threshold_at_m[m] = candidate_threshold\n",
    "\n",
    "print(\"Best threshold at m=\" + str(m) + \":\", best_threshold_at_m[m])\n",
    "\n",
    "m = m2\n",
    "prob_column = \"prob_yes_\" + str(m)    \n",
    "best_reward = -10000000\n",
    "for candidate_threshold in np.linspace(val_with_predictions[prob_column].min()-10**-12,\n",
    "                                       val_with_predictions[prob_column].max()+10**-12,\n",
    "                                       num=num_grid_points):\n",
    "    \n",
    "    total_reward, average_reward, total_sales, total_time =\\\n",
    "        simulate_threshold(candidate_threshold, best_threshold_at_m[m1], val_with_predictions)\n",
    "    \n",
    "    if average_reward > best_reward:\n",
    "        best_reward = average_reward\n",
    "        best_threshold_at_m[m] = candidate_threshold\n",
    "\n",
    "print(\"Best threshold at m=\" + str(m) + \":\", best_threshold_at_m[m])\n",
    "\n",
    "total_reward_agent, average_reward_agent, \\\n",
    "    total_sales_agent, total_time_agent = simulate_threshold(best_threshold_at_m[m1],\n",
    "                                                             best_threshold_at_m[m2],\n",
    "                                                             test_with_predictions)\n",
    "\n",
    "print()\n",
    "print(\"Test set reward with the stopping agent:\")\n",
    "print(\"Total reward on test:\", total_reward_agent)\n",
    "print(\"Avg. reward on test:\", average_reward_agent)\n",
    "print(\"Total sales on test:\", total_sales_agent)\n",
    "print(\"Total time on test:\", total_time_agent)\n",
    "print()\n",
    "\n",
    "print(\"Comparative results:\")\n",
    "print(\"Sales lost by stopping agent:\", total_sales_noagent - total_sales_agent)\n",
    "print(\"Time saved by stopping agent (seconds):\", total_time_noagent - total_time_agent)\n",
    "print(\"Time saved by stopping agent (%):\", (total_time_noagent - total_time_agent)/\\\n",
    "                                            total_time_noagent * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
